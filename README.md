# Simple Node.js Website Crawler

Този проект е базов уеб crawler, написан на Node.js. Той рекурсивно обхожда зададен уебсайт, проверява HTTP статус кодовете на страниците и записва всички грешки (напр. 404, 500) в текстов файл `errors_report.txt`.

## Характеристики

- Рекурсивно обхождане на сайта, като следи само вътрешни линкове (същия домейн)  
- Проверка на HTTP статус кодове за всяка страница  
- Записва грешки и проблеми в лесно четим текстов файл  
- Използва `axios` за HTTP заявки и `cheerio` за парсване на HTML и извличане на линкове  

## Изисквания

- Node.js (препоръчително версия 14+)  
- npm  

## Инсталация и стартиране

1. Клонирай репото
2. Инсталирай зависимостите
3. Стартирай crawler-а с начална страница:
`node crawler.js https://example.com`
4. След приключване грешките ще са записани в `errors_report.txt`.

## Забележки
Проектът е базов и може да бъде надграждан с лимити, филтри, многопоточност и други подобрения.
Препоръчва се да не се обхождат прекалено големи сайтове наведнъж, за да не се претовари сървърът.
